{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXl0ASQ6dFh"
      },
      "source": [
        "# **Titre du TP : Introduction à la Classification d'Images satellitaires avec les Réseaux Neuronaux Convolutifs (CNN)**\n",
        "\n",
        "**Description :**\n",
        "\n",
        "Dans ce TP, nous explorerons les concepts fondamentaux liés à la classification d'images en utilisant des Réseaux Neuronaux Convolutifs (CNN). Notre objectif principal sera de catégoriser des images satellites en fonction du type de terrain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4yyefqm6sxD"
      },
      "source": [
        "\n",
        "**Étapes du TP :**\n",
        "\n",
        "Dans ce TP nous allons apprendre à manipuler un réseau de neurones convolutionnel pour une tache de classification afin catégoriser des d'images satellites en fonction du type de terrain.\n",
        "\n",
        "Nous allons progresser en suivant les étapes suivantes :\n",
        "\n",
        "**Partie 1 :** Préparation de l'environnement de travail\n",
        "\n",
        "1. **Bibliothéques Deep Leraning :** Utilisation de la libraire `fastai`\n",
        "2. **Données images :** Utilisation d'images EuroSAT RGB\n",
        "\n",
        "**Partie 2 :** Appropriation\n",
        "\n",
        "1. **Utiliser un modèle Deep-Learning pré-entrainé** :  Apprendre (1) à charger un modèle de réseau de neurones existant et (2) à exécuter une tache de classification sur une images de notre choix téléchargée.\n",
        "\n",
        "2. **Manipuler un dataset** : Apprendre (1) à charger un dataset d'images dans depuis le disque pour entrainer un modèle ; (2) Savoir comment explorer les données d'un dataset.\n",
        "\n",
        "3. **Voir la structure d'un modèle** :  Apprendre (1) à créer lun modèle basé sur une architecture type (resnet50) et (2) à visualiser toutes les couches du réseau avec leurs détails.\n",
        "\n",
        "4. **Entrainer un modèle**  Apprendre (1) à réaliser une phase d'apprentissage d'un modèle ; (2) comprendre comment sauvegarder un modèle avec ces paramètres d'apprentissage ; (3) apprendre à visualiser la précision de prédiction de notre modèle et enfin (4) voir comment examiner les résultats et les erreurs du modèle.\n",
        "\n",
        "**Partie 3 :** Réalisation\n",
        "\n",
        "1. **Classifier une photo satellitaire choisie** :\n",
        "Trouver une photo satellitaire de la Réunion, telecharger là et utiliser notre modèle pour la classifier.\n",
        "\n",
        "2. **Enrichir un modèle** :\n",
        "Enrichir les classes du modèle pour prendre en considération d'autres types de terrain.\n",
        "\n",
        "**Partie 4 :** Bonus\n",
        "\n",
        "1. Pour les étudiants avancés, reproduire les élements de ce TP avec la bibliothèque `pytorch` qui necessite un peu plus d'entrer dans l'architecture du réseau de neurones.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT2Cr96BbBCW"
      },
      "source": [
        "# *Préparation du TP :*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un6k9ZOL6jvr"
      },
      "source": [
        "### 1. Bibliothéques Deep Leraning :\n",
        "Dans ce TP nous allons utiliser la bibliothèque **`fastai`** qui est une bibliothèque de programmation en Python qui simplifie l'utilisation des techniques de Deep Learning. Elle est construite sur le framework PyTorch, une autre bibliothèque de machine learning populaire de plus bas niveau.\n",
        "\n",
        "Commençons par importer cette librairie :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf7qEwOn6eCo"
      },
      "outputs": [],
      "source": [
        "import fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# *Partie 3 du TP: Réalisation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# *Partie 4 du TP : Bonus*\n",
        "\n",
        "Pour les étudiants avancés, reproduire les élements de ce TP avec la bibliothèque PyTorch qui nécessite un peu plus de rentrer dans les détails de l'entrainement du réseau de neurones.\n",
        "\n",
        "Pytorch permet d'avoir plus de flexibilité dans l'entraînement, et de nombreux choix de design ne sont pas fait par défaut et nous reviennent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Votre code pour l'exercice \"bonus\" ici ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr9AeEhi9sAR"
      },
      "source": [
        "### 2. Données images :\n",
        "\n",
        "Dans ce TP, nous utiliserons les images EuroSAT RGB depuis le site web du DKFI (Centre de recherche allemand spécialisé dans l'intelligence artificielle)\n",
        "Les images sont compressées dans un fichier zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL_nEKTV6gk1"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import untar_data, Path\n",
        "path = untar_data('http://madm.dfki.de/files/sentinel/EuroSAT.zip')\n",
        "Path.BASE_PATH = path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ8qOXirb_sm"
      },
      "source": [
        "Voyons les images EuroSAT RGB à disposition pour ce TP : On constate que les images sont organisées dans des répertoires en fonction de ce qu'elle représente (leur étiquette)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xWrET37liBa"
      },
      "outputs": [],
      "source": [
        "print(path.ls())\n",
        "print((path / 'HerbaceousVegetation').ls()[:10])\n",
        "\n",
        "images = list(path.rglob(\"*.jpg\"))\n",
        "print(len(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKmnYvzT8HTH"
      },
      "source": [
        "# *Partie 2 du TP: Appropriation*\n",
        "## 1. **Utiliser un modèle Deep-Learning pré-entraîné**\n",
        "\n",
        "Dans un premier temps on va voir comment avec la librairie `fastai`, il est possible d'utiliser un modèle déjà entrainé pour une tache bien spécifique de classification.\n",
        "\n",
        "En effet, il se peut que vous disposiez d'un modèle pré-entrainé ayant atteint un bon niveau de qualité de prédiction et que vous souhaitez l'utiliser directement pour analyser des images satellites et les classifier rapidement.\n",
        "\n",
        "Pour cela dans cette section nous allons :\n",
        "1. Apprendre à charger un modèle de reseau de neurones existant\n",
        "2. Télécharger une image à traiter (à classifier)\n",
        "2. Apprendre à exécuter une tache de classification sur l'image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d36A6Md0LDYb"
      },
      "source": [
        "### 1.1. Charger un modèle pré-entraîné\n",
        "\n",
        "Nous téléchargeons depuis github un fichier nommé `two_classes_model.pkl` contenant un modèle entrainé.  \n",
        "Pour cela, nous utilisons la commande `wget`.  \n",
        "Ce ficher contient le modèle entraîné pour classer des images satellites en fonction de deux types de terrain: **Nature** ou **Humain**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfqzwbVz5pML"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/courdier/ia/raw/main/TP/pretrained_models/two_classes_model.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0BuvNmCPg5D"
      },
      "source": [
        "Pour charger ce modèle, on utilise la fonction `load_learner`de `fastai`.\n",
        "\n",
        "**Note :** *Un fichier pickle \".pkl\" est un format en Python qui permet de sauvegarder des objets Python complets (sérialisation).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRzwGMVEBE21"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import load_learner\n",
        "model = load_learner('two_classes_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKl0B8KGiw9k"
      },
      "source": [
        "### 1.2. Ouvrir une image à traiter\n",
        "\n",
        "\n",
        " Choisissons parmis nos données du TP définies plus haut l'image à traiter.<br>\n",
        " Il s'agira en l'occurence d'une image de végétation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy4PCub5jFdI"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = Image.open(path/'HerbaceousVegetation/HerbaceousVegetation_450.jpg')\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwTQKd2DHX2o"
      },
      "source": [
        "### 1.4. Exécuter une tache de classification sur l'image\n",
        "Appliquons notre modèle sur cette image pour voir s'il permet d'identifier correctement la classe de notre image.\n",
        "Pour cela, nous allons appeler la méthode `predict(image)` sur notre modèle.\n",
        "\n",
        "Les résultats de la prédiction sont stockés dans trois variables :\n",
        "* `pred` :     La classe identifiée.\n",
        "* `pred_idx` : L'indice de la classe indentifiée.\n",
        "* `probs` :    Les probabilités associées à chaque classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxrcZjQ_irxo"
      },
      "outputs": [],
      "source": [
        "# Réalisatin de la prédiction en utilisant le modèle (model) sur une image donnée (image).\n",
        "pred,pred_idx,probs = model.predict(image)\n",
        "\n",
        "# Affichage de la classe identifiée et la probabilité associée\n",
        "f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNGRT1GhxXe9"
      },
      "source": [
        "### 1.5. Discussion\n",
        "\n",
        "Un modèle pre-entrainé peut ne pas correspondre totalement à notre attente spécifique. Il est parfois nécessaire d'affiner un modèle déjà pré-entrainer avec notre propre jeu de données pour augmenter la qualité de ses prédictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAvbWwTN8qc3"
      },
      "source": [
        "## **2. Manipuler un dataset**\n",
        "\n",
        "Le Dataset correspond aux données initailes nécessaire, ces données sont  le point de départ de tout algorithme de Machine Learning.  \n",
        "C'est autant la qualité que la quantité des données qui est importante.\n",
        "\n",
        "Dans cette section nous allons :\n",
        "\n",
        "1. Voir comment télécharger un dataset d'images ;\n",
        "2. Apprendre à creer un dataloader qui est une structures de données organisant le dataset dans fastai;\n",
        "3. Savoir comment explorer les données de ce dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wsEJtshla1m"
      },
      "source": [
        "### 2.1 Téléchargement des données\n",
        "\n",
        "Cette opération a été réalisée dans la partie préparation du TP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M44PfSNt7Wck"
      },
      "source": [
        "### 2.2 Création du dataloader pour organiser les données du dataset avec fastai\n",
        "\n",
        "Le `Dataloader` est un objet qui permet de charger et organiser les données qui constituent le dataset. <BR>Ce dataloader sera utilisée pour entrainer le modèle.\n",
        "\n",
        "Il permet de :\n",
        "- charger les données en parallèle (et non sequentiellement)\n",
        "- créer un batch de données (un ensemble de données qui seront traitées en même temps par le modèle)\n",
        "- mélanger les données aléatoirement\n",
        "- appliquer des transformations sur les données, par exemple pour les mettre dans le bon format\n",
        "\n",
        "Dans `fastai`, le dataloader est créé à partir d'un objet appelé `DataBlock`, qui permet de définir comment les données sont chargées et transformées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4AWo1a26cLc"
      },
      "outputs": [],
      "source": [
        "# Importation des modules nécessaires de fastai\n",
        "from fastai.vision.all import DataBlock, ImageBlock, CategoryBlock, RandomSplitter, RegexLabeller\n",
        "from fastai.vision.all import get_image_files, using_attr, aug_transforms\n",
        "\n",
        "# La fonction label_func prend en paramètre le nom d'un fichier (fname) et retourne le label associé ('Nature' ou 'Human') en fonction des classes spécifiées.\n",
        "def label_func(fname):\n",
        "    # Liste des classes appartenant à \"zones naturelles\"\n",
        "    nature_classes = ['Forest', 'River', 'AnnualCrop', 'HerbaceousVegetation', 'PermanentCrop', 'SeaLake', 'Pasture']\n",
        "    # Vérifie si le nom du fichier contient l'une des classes de nature\n",
        "    if any(nc in fname.name for nc in nature_classes):\n",
        "        return 'Nature'\n",
        "    else:\n",
        "        return 'Human'\n",
        "\n",
        "# Définition d'un objet blocks de type DataBlock\n",
        "blocks = DataBlock(\n",
        "  blocks = (ImageBlock, CategoryBlock),               # spécifie les types de blocs de données à utiliser\n",
        "  get_items = get_image_files,                        # définit la fonction qui va récupérer les fichiers images\n",
        "  splitter = RandomSplitter(valid_pct=0.2, seed=0),   # définit la fonction qui va séparer les données en train et valid\n",
        "  get_y = label_func,                                 # définit la fonction qui va extraire les labels des données\n",
        "  batch_tfms = aug_transforms(mult=2)                 # définit la fonction qui augmente la diversité des données.\n",
        ")\n",
        "\n",
        "# Création du data loader\n",
        "# Cet objet dataloader contient les données prétraitées prêtes à être utilisées dans un modèle d'apprentissage automatique.\n",
        "dataloader = blocks.dataloaders(path)\n",
        "\n",
        "# Affichage de la taille des ensembles d'entraînement et de validation\n",
        "len(dataloader.train_ds), len(dataloader.valid_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUB_pD0AuC-L"
      },
      "source": [
        "Détails des arguments:\n",
        "\n",
        "- `blocks = (ImageBlock, CategoryBlock)` :\n",
        "Cela spécifie les types de blocs de données à utiliser. Dans ce cas, il y a deux blocs : `ImageBlock` pour les données d'image et `CategoryBlock` pour les données de catégorie (étiquettes).\n",
        "\n",
        "- `get_items = get_image_files` :\n",
        "Cela définit la fonction qui sera utilisée pour récupérer les données. `get_image_files` est une fonction qui extrait les chemins des fichiers d'images à partir d'un répertoire donné.\n",
        "\n",
        "- `splitter = RandomSplitter(valid_pct=0.2, seed=0)` :\n",
        "Cela définit la fonction qui sépare les données en ensembles d'entraînement et de validation. Dans ce cas, `RandomSplitter` est utilisé pour diviser les données de manière aléatoire. `valid_pct=0.2` indique que 20 % des données seront réservées pour l'ensemble de validation, et seed=0 définit la seed utilisée pour la reproductibilité. (Une seed est un nombre utilisé pour l'initialisation d'un générateur de nombres pseudo-aléatoires).\n",
        "\n",
        "- `get_y = label_func` :\n",
        "Cela définit la fonction qui extrait les étiquettes (labels) à partir des noms de fichiers d'images.\n",
        "\n",
        "- `batch_tfms = aug_transforms(mult=2)` :\n",
        "Cela définit les transformations à appliquer sur les données en mini-lots (batches) lors de l'entraînement. `aug_transforms` spécifie que l'on va appliquer des \"augmentations de données\" qui vont modifier legèrement les images de l'ensemble d'entrainement, ce qui est souvent utile pour améliorer la généralisation du modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSx2Ahzw7WVT"
      },
      "source": [
        "### 2.3 Exploration des données\n",
        "nnes.Affichage d'une grille d'échantillons d'images du jeu de données avec leurs étiquettes associées. Chaque cellule de la grille contiendra une image, et la grille sera composée de deux lignes et cinq colo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaeCT5jX_ie1"
      },
      "outputs": [],
      "source": [
        "dataloader.show_batch(nrows=2, ncols=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryS_ilDfF-mq"
      },
      "source": [
        "La ligne de code suivante démontre comment `aug_transforms` enrichit les données. Elle affiche une image de l'ensemble d'entraînement, révélant de multiples possibles augmentations que peut réaliser par le dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msnpOV2qBBtN"
      },
      "outputs": [],
      "source": [
        "dataloader.train.show_batch(nrows=3, unique=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1XfqXNFo-MO"
      },
      "source": [
        "## **3. Structure d'un modèle**  \n",
        "\n",
        "Maintenant que nous avons notre structure de dataset, nous allons créer le réseau de neurones sur lequel nous allons réaliser l'entrainement.\n",
        "\n",
        "Dans cette section, nous allons :\n",
        "\n",
        "1. Apprendre à créer un modèle basé sur une architecture type (resnet50)\n",
        "2. Apprendre à visualiser toutes les couches du réseau avec leurs détails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UZXpuMbsX7J"
      },
      "source": [
        "### 3.1. Création du modèle basé sur une architecture type\n",
        "\n",
        "Nous allons utiliser la fonction `vision_learner` de `fastai` pour creer le modele en lui demandant une architecture de type `resnet50`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqQ9RrFypDkT"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import vision_learner, resnet50, accuracy\n",
        "\n",
        "model = vision_learner(dataloader, resnet50, metrics=accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSOH1rKBpMqx"
      },
      "source": [
        "### 3.2. Visualisation de toutes les couches du réseau avec leurs détails.\n",
        "\n",
        "Il est possible de voir la constitution du modèle grâce à la fonction `model.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijy8xi-epg42"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRxkl6qrShca"
      },
      "source": [
        "Première partie de l'architecture :\n",
        "\n",
        "**Entrée (Input shape: 64 x 3 x 64 x 64) :**\n",
        "* Le batch d'entrée est de taille 64.\n",
        "* Il a 3 canaux (pour une image RGB).\n",
        "* La taille de chaque image est de 64x64 pixels.\n",
        "\n",
        "**Bloc 1 :**\n",
        "* Conv2d: Couche de convolution avec 64 filtres, chaque image en sortie du bloc est de taille 32x32.\n",
        "* BatchNorm2d: Normalisation par lot pour les 64 canaux. Permet d'accélérer la convergence et d'améliorer la stabilité du modèle.\n",
        "* ReLU: Fonction d'activation ReLU -> introduction de la non-linéarité\n",
        "\n",
        "**...**\n",
        "\n",
        "La dernière partie de l'architecture décrit le processus de classification finale :\n",
        "\n",
        "**Bloc Adaptive Average Pooling et Adaptive Max Pooling**\n",
        "\n",
        "* Adaptive Average Pooling et Adaptive Max Pooling : ces fonctions calculent la moyenne et le maximum des valeurs sur les dimensions spatiales, pour réduire les dimensions spatiales de la sortie de (64 x 2048 x 2 x 2) à 64 x 2048 x 1 x 1.\n",
        "\n",
        "**Bloc d'applatissement des couches et de normalisation**\n",
        "\n",
        "L'aplatissement et les couches de normalisation par lot et de dropout fournissent une représentation plate et régularisée qui peut être utilisée pour la classification finale.\n",
        "* Flatten est utilisé pour convertir les données en un format unidimensionnel. ici elle transforme la forme 64 x 2048 x 1 x 1 en une forme plate de 64 x 4096.\n",
        "* BatchNorm1d: Normalisation par lot pour les 4096 sorties après l'aplatissement. Permet d'accélérer la convergence et d'améliorer la stabilité du modèle.\n",
        "* Dropout: Couche de régularisation on ignore certaines sorties de neurones pour prévenir le surapprentissage.\n",
        "\n",
        "**Couche entièrement connectée (Fully Connected Layer) :**\n",
        "* Linear: Couche linéaire avec 512 sorties.\n",
        "* ReLU: Fonction d'activation ReLU.\n",
        "* BatchNorm1d: Normalisation par lot pour les 512 sorties.\n",
        "* Dropout: Couche de dropout pour régularisation.\n",
        "\n",
        "**Couche de sortie :**\n",
        "* Linear: Couche linéaire avec 10 sorties pour les 10 classes de notre tâche de classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Mjhi9Pqtfn"
      },
      "source": [
        "**Modèle avec partie figées pendant l'entraînement :**\n",
        "\n",
        ">Avec **Fastai**, lorsque vous créez un modèle en utilisant la fonction `vision_learner`, le comportement par défaut est de charger un modèle pré-entrainé et de geler toutes les couches sauf les couches de classification finales.\n",
        "Cela signifie que les poids pré-entraînés du modèle, dans notre cas le resnet50, restent figés. Seuls les poids des couches de classification finales sont mis à jour pendant l'entraînement.\n",
        "\n",
        "**La section \"Total params\" :**\n",
        ">Remarquez à la fin le nombre de paramètres entrainables : La section \"Total params\" donne le nombre total de paramètres dans le réseau ; **plus de 25 millions** (25 619 520) et le nombre de paramètres entraînables ; un peu plus de 2 millions (2 164 608). Ce nombre de paramètre confirme que le modèle resnet50 utilisé ici est bien un réseau profond.\n",
        "\n",
        "On voit bien ici par élémenst que la plus grande partie des paramètres de notre modèle sont figés. Il s'agit d'une pratique courante lorsque l'on fait du \"**fine-tuning**\", c'est-à-dire lorsque que l'on réutilise des modèles pré-entraîné sur une tâche similaire à celle que l'on veut réaliser sur nos données.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpCfZ5nBh6u"
      },
      "source": [
        "Pour examiner en détail le modèle sous-jacent dans `PyTorch`, on peut employer la commande `model.model`.\n",
        "Cette méthode offre un aperçu plus approfondi que `model.summary()`, car elle détaille la structure interne du modèle, permettant ainsi une meilleure compréhension de l'agencement et de la fonctionnalité de ses différentes couches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAkuKcI7AKPa"
      },
      "outputs": [],
      "source": [
        "model.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEgnpUZqLWv3"
      },
      "source": [
        "## **4. Entrainer un modèle**\n",
        "\n",
        "Maintenant que nous avons un dataset et un modèle de réseau de neurones nous pouvons passer à la phase d'entrainement du modèle.\n",
        "Cette phase d'entraînement implique l'ajustement itératif des poids du réseau pour minimiser une fonction de coût.\n",
        "\n",
        "Dans cette section nous allons :\n",
        "\n",
        "1. Apprendre à réaliser une phase d'apprentissage d'un modèle ;\n",
        "2. Voir comment sauvegarder un modèle avec ces paramètres d'apprentissage ;\n",
        "3. Apprendre à visualiser la précision de prédiction de notre modèle\n",
        "4. Comprendre comment examiner les résultats et les erreurs du modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwiwJD3GsvH-"
      },
      "source": [
        "### 4.1. La phase d'apprentissage\n",
        "\n",
        "Il suffit d'appeler la fonction `fine_tune` du modèle pour lancer l'apprentissage.\n",
        "\n",
        "La fonction prend en paramètre le nombre d'*epoch* à faire.  \n",
        "Une *epoch* correspond à une passe à travers tout le dataset, c'est-à-dire au fait que le modèle a utilisé une fois toutes les images du dataset.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfQ0vTbQpt2T"
      },
      "outputs": [],
      "source": [
        "model.fine_tune(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8chSOa5hfH9s"
      },
      "source": [
        "Analyse de cette sortie\n",
        "\n",
        "* **Epoch** : Une époque représente une itération complète à travers l'ensemble de données d'entraînement.\n",
        "* **train_loss** : La perte (loss) sur l'ensemble d'entraînement à la fin de chaque époque. Il mesure à quel point les prédictions du modèle diffèrent des vraies étiquettes pour les données d'entraînement. Une diminution de cette valeur indique une amélioration du modèle.\n",
        "Cette loss est utilisée durant le training pour calculer les gradients et améliorer le modèle.\n",
        "* **valid_loss** : Similaire à train_loss mais calculé sur l'ensemble de validation.\n",
        "* **accuracy** : La précision (accuracy) sur l'ensemble de validation à la fin de chaque époque. Il représente la proportion correcte de prédictions par rapport au nombre total d'images dans l'ensemble de validation.\n",
        "* **time** : Le temps écoulé pendant cette époque d'entraînement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxNsR6-vf8gv"
      },
      "source": [
        "### 4.2. Sauvegarde de notre modèle avec ces paramètres d'apprentissage\n",
        "\n",
        "Vous pouvez sauvegarder le modele en utilisant la fonction `export`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYi_b-saWFwU"
      },
      "outputs": [],
      "source": [
        "model.export(\"model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UL6g18EXz9"
      },
      "source": [
        "Une fois le modèle sauvegardé vous pourrez l'importer à partir du fichier exporté pour effectuer des prédictions sans avoir à le ré-entraîner à chaque fois. Pour cela, vous pouvez utiliser la fonction `model = load_learner('model.pkl')`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLwCHjc-sLRo"
      },
      "source": [
        "### 4.3. Visualiser la précision de prédiction de notre modèle\n",
        "\n",
        "**fastia** fournit un recorder, sorte d'enregistreur qui stocke différentes métriques et informations pendant l'entraînement du modèle.\n",
        "On peut afficher la courbe d'évolution de l'accuracy (la précision) en utilisant ce code :\n",
        "\n",
        "* L(...) est une construction de liste dans Fastai, qui est souvent utilisée pour créer une liste à partir d'une séquence (dans ce cas, probablement une liste ou un tableau).\n",
        "\n",
        "* itemgot(2) est une méthode de liste qui extrait l'élément à l'index 2 de chaque élément de la liste. l'index 2 de la liste construite par le recordeur  est ici associé à la précision (accuracy).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK2Lcq0CsJwy"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import plt, L\n",
        "\n",
        "accuracy_values = L(model.recorder.values).itemgot(2)\n",
        "plt.plot(accuracy_values);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wERlVh57sLtA"
      },
      "source": [
        "### 4.4. Examiner les résultats et les erreurs du modèle\n",
        "\n",
        "Pour comprendre le type d'erreur que notre modèle fait, nous pouvons utiliser ce que l'on appelle une **matrice de confusion**. Celle-ci montre combien d'images d'une classe donnée ont été mal classifiées dans chacune des autres classes.\n",
        "\n",
        "En effet, il est connu par exemple que la réponse spectrale des rivières (eaux troubles) et des routes est assez similaire notablement dans le spectre visible, puisqu'ils absorbent une grande partie du rayonnement solaire.  \n",
        "Vu de loin, dans une zone de 640 m x 640 m, une rivière et une autoroute peuvent donc être confondues. Par conséquent, il ne serait pas surprenant que certaines images de rivières et de routes soient incorrectement classifiées, signifiant qu'une image de rivière pourrait être interprétée comme celle d'une route et inversement par notre modèle.\n",
        "\n",
        "Nous utilisons la matrice de confusion calculée sur les données de validation pour identifier les classes où des erreurs de classification se sont produites.\n",
        "Cette  matrice montre le nombre d'échantillons correctement classés (diagonale principale) et les erreurs de classification pour chaque classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd7U30CLsJzg"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import ClassificationInterpretation\n",
        "\n",
        "interpretation = ClassificationInterpretation.from_learner(model)\n",
        "interpretation.plot_confusion_matrix(figsize=(12,12), dpi=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM4TQg-s3w_p"
      },
      "source": [
        "Nous pouvons utiliser la fonction `most_confused` pour voir quelles sont les erreurs les plus fréquentes de notre modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsNUou0i3xHb"
      },
      "outputs": [],
      "source": [
        "interpretation.most_confused(min_val=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YISuYO9XbYO4"
      },
      "source": [
        "# *Partie 3 du TP: Réalisation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1. **Classifier une image satellitaire choisie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbh_cPUSU3ha"
      },
      "source": [
        "### **Exercice 1 :** Utiliser notre modèle avec vos propres images\n",
        "\n",
        "Il vous est demandé ici :\n",
        "\n",
        "1. de trouver quelques photos satellitaires  \n",
        "sur lesquels nous pourrons appliquer notre modèle de classification.\n",
        "\n",
        "2. d'écrire le code python permettant de choisir et télécharger une image à traiter. <BR>\n",
        "Pour cela, utilisez l'utilitaire `widget` définie dans `fastai.vision.widgets` et qui permet le téléchargement d'une image avec l'instruction `widgets.FileUpload()`.\n",
        "\n",
        "3. D'appliquer notre modèle à cette image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHUUM0_xn07_"
      },
      "source": [
        "### Exercice 1. - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDtHTM1OsJ1s"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.widgets import *\n",
        "\n",
        "# Création du bouton widget pour le téléchargement\n",
        "btn_upload = widgets.FileUpload()\n",
        "btn_upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUG2XFUj4TJD"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import PILImage\n",
        "\n",
        "# l'image téléchargée est stockée dans la variable 'image'\n",
        "image = PILImage.create(btn_upload.data[-1])\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdI6TKXp4jgi"
      },
      "outputs": [],
      "source": [
        "pred, pred_idx, probs = model.predict(image)\n",
        "f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD-dYXcz5C_X"
      },
      "source": [
        "\n",
        "## 3.2. **Enrichir un modèle : Enrichir les classes du modèle pour prendre en considération d'autres types de terrain.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Exercice 2.** Enrichir ce modèle en affinant les classes à détecter **\n",
        "\n",
        "Notre modèle a été pré-entrainé pour caractériser des images dans les classes :\n",
        "- Nature\n",
        "- Humain\n",
        "\n",
        "Il vous est demandé de pouv oir maintenant considérer les classes suivantes :\n",
        "  - Forest,\n",
        "  - River,\n",
        "  - AnnualCrop,\n",
        "  - HerbaceousVegetation,\n",
        "  - Residential,\n",
        "  - PermanentCrop,\n",
        "  - Industrial,\n",
        "  - SeaLake,\n",
        "  - Pasture,\n",
        "  - Highway\n",
        "\n",
        "  Pour cela :\n",
        "  \n",
        "  **a)** Définissez une nouvelle fonction d'extraction des labels de données applicable au dataset des images EuroSAT,  \n",
        "  **b)** Testez en montrant que votre dataloader a bien étiqueté vos images de votre dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Vh92JLsCSb"
      },
      "source": [
        "### EXERCICE 2. - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fastai'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/remycourdier/Sites/wwwCours/ia/ia/TP/TP_1_GIS_Corrected.ipynb Cellule 64\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remycourdier/Sites/wwwCours/ia/ia/TP/TP_1_GIS_Corrected.ipynb#Y140sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# import initiaux \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/remycourdier/Sites/wwwCours/ia/ia/TP/TP_1_GIS_Corrected.ipynb#Y140sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfastai\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remycourdier/Sites/wwwCours/ia/ia/TP/TP_1_GIS_Corrected.ipynb#Y140sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfastai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall\u001b[39;00m \u001b[39mimport\u001b[39;00m untar_data, Path\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remycourdier/Sites/wwwCours/ia/ia/TP/TP_1_GIS_Corrected.ipynb#Y140sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfastai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall\u001b[39;00m \u001b[39mimport\u001b[39;00m vision_learner, resnet50, accuracy\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastai'"
          ]
        }
      ],
      "source": [
        "# import initiaux \n",
        "import fastai\n",
        "from fastai.vision.all import untar_data, Path\n",
        "from fastai.vision.all import vision_learner, resnet50, accuracy\n",
        "from fastai.vision.all import plt, L\n",
        "from fastai.vision.all import ClassificationInterpretation\n",
        "\n",
        "path = untar_data('http://madm.dfki.de/files/sentinel/EuroSAT.zip')\n",
        "Path.BASE_PATH = path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ9WM6cvp9WE"
      },
      "outputs": [],
      "source": [
        "# a). Nouvelle fonction d'extraction des labels de données.\n",
        "# ======================================================================\n",
        "#   Création du data block permettant de paramétrer le dataloader à sa création\n",
        "#   On utilise `RegexLabeller` pour extraire les étiquettes à partir de noms de fichiers correspondant à un motif régulier (regex).\n",
        "#   Dans notre cas, cette expression régulière capture une partie du nom du fichier qui se trouve entre le début de la chaîne\n",
        "#   et le dernier souligné avant une séquence de chiffres suivie de \".jpg\". Cette partie capturée est utilisée comme label pour les données.\n",
        "#   Par exemple, si le nom du fichier est \"nature_123.jpg\", le label extrait serait \"nature\".\n",
        "\n",
        "blocks = DataBlock(\n",
        "  blocks = (ImageBlock, CategoryBlock),\n",
        "  get_items = get_image_files,\n",
        "  splitter = RandomSplitter(valid_pct=0.2, seed=0),\n",
        "  get_y = using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),  # *** Nouvelle fonction d'extraction des labels de données\n",
        "  batch_tfms = aug_transforms(mult=2)\n",
        ")\n",
        "\n",
        "# Création du data loader\n",
        "dataloader = blocks.dataloaders(path)\n",
        "\n",
        "# b). Test\n",
        "# ========\n",
        "dataloader.show_batch(nrows=2, ncols=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Exercice 3.** Entrainement de votre modéle, test avec vos images et analyse des résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkgVJJg4w-P-"
      },
      "source": [
        "### Exercice 3. - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2qNYSYDxBPV"
      },
      "outputs": [],
      "source": [
        "# Entrainement du modèle\n",
        "# ======================================================================\n",
        "\n",
        "# creer le modele en lui demandant une architecture de type `resnet50`\n",
        "model = vision_learner(dataloader, resnet50, metrics=accuracy)\n",
        "\n",
        "# lancer l'apprentissage 3 fois sur le dataset \n",
        "model.fine_tune(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des données suite à l'entrainement\n",
        "# ======================================================================\n",
        "\n",
        "accuracy_values = L(model.recorder.values).itemgot(2)\n",
        "plt.plot(accuracy_values);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examinun des résultats et les erreurs du modèle\n",
        "\n",
        "interpretation = ClassificationInterpretation.from_learner(model)\n",
        "interpretation.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
        "interpretation.most_confused(min_val=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4kMHbAG4uqr"
      },
      "source": [
        "# *Partie 4 du TP : Bonus*\n",
        "\n",
        "Pour les étudiants avancés, reproduire les élements de ce TP avec la bibliothèque PyTorch qui nécessite un peu plus de rentrer dans les détails de l'entrainement du réseau de neurones.\n",
        "\n",
        "Pytorch permet d'avoir plus de flexibilité dans l'entraînement, et de nombreux choix de design ne sont pas fait par défaut et nous reviennent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMO46jZjx2jc"
      },
      "source": [
        "### Exercice \"Bonus\" - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK45ZF0VRqxM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download and unzip data\n",
        "url = 'http://madm.dfki.de/files/sentinel/EuroSAT.zip'  # Define the URL of the dataset zip file\n",
        "response = requests.get(url)  # Send an HTTP GET request to download the zip file\n",
        "zip_file = ZipFile(BytesIO(response.content))  # Create a ZipFile object from the downloaded content\n",
        "zip_file.extractall('EuroSAT')  # Extract all the contents of the zip file to the 'EuroSAT' directory\n",
        "path = '/content/EuroSAT/'  # Set the path to the extracted dataset directory\n",
        "\n",
        "# Custom Dataset Class\n",
        "class EuroSATDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir  # Set the root directory where the dataset is located\n",
        "        self.transform = transform or (lambda x: x)  # Define a data transformation (or use the identity transformation)\n",
        "        self.images = list(Path(root_dir).rglob(\"*.jpg\"))  # Create a list of image file paths in the dataset\n",
        "        self.class_to_idx = {'Human': 0, 'Nature': 1}  # Define a mapping of class names to class indices\n",
        "        self.class_to_idx_2 = {'Forest': 0, 'River': 1, 'AnnualCrop': 2, 'HerbaceousVegetation': 3, 'PermanentCrop': 4, 'SeaLake': 5, 'Pasture': 6, 'Highway': 7, 'Industrial': 8, 'Residential': 9}  # Define a more detailed class mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)  # Return the total number of images in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.images[idx])  # Get the file path of the image\n",
        "        image = self.transform(Image.open(img_name))  # Apply the transformation to the image\n",
        "        label = self.label_func(img_name)  # Get the label for the image\n",
        "        return image, label  # Return the transformed image and its label\n",
        "\n",
        "    def label_func(self, fname):\n",
        "        nature_classes = ['Forest', 'River', 'AnnualCrop', 'HerbaceousVegetation', 'PermanentCrop', 'SeaLake', 'Pasture']\n",
        "        label = 'Nature' if any(nc in fname for nc in nature_classes) else 'Human'  # Determine if the image belongs to the 'Nature' or 'Human' class based on its file path\n",
        "        return self.class_to_idx[label]  # Return the class index\n",
        "\n",
        "    def label_func_2(self, fname):\n",
        "        label = fname.split(\"/\")[-1].split(\"_\")[0]  # Extract the class label from the image file path\n",
        "        return self.class_to_idx_2[label]  # Return the class index\n",
        "\n",
        "# Transformations (basic one only)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image pixel values\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = EuroSATDataset(path, transform=transform)  # Create an instance of the EuroSATDataset with the specified transformation\n",
        "train_size = int(0.8 * len(dataset))  # Calculate the size of the training set (80% of the dataset)\n",
        "valid_size = len(dataset) - train_size  # Calculate the size of the validation set (remaining 20% of the dataset)\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])  # Split the dataset into training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Create a DataLoader for the training set\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)  # Create a DataLoader for the validation set\n",
        "\n",
        "# Model\n",
        "model = models.resnet50(pretrained=True)  # Load a pre-trained ResNet-50 model\n",
        "num_ftrs = model.fc.in_features  # Get the number of input features to the final fully connected layer\n",
        "model.fc = torch.nn.Linear(num_ftrs, 2)  # Replace the final fully connected layer with a new one for binary classification (2 classes: Nature and Human)\n",
        "\n",
        "model.requires_grad_(False)  # Set all model parameters to not require gradients\n",
        "model.fc.requires_grad_(True)  # Set the parameters of the final fully connected layer to require gradients\n",
        "\n",
        "# Training setup\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define the loss function (Cross-Entropy Loss)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Define the optimizer (Adam) and learning rate\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if a GPU is available and set the device accordingly\n",
        "\n",
        "# Training loop\n",
        "model.to(device)  # Move the model to the selected device (CPU or GPU)\n",
        "for epoch in range(3):  # Loop over a fixed number of training epochs (3 in this case)\n",
        "    model.train()  # Set the model to training mode\n",
        "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{3}', unit='batch'):  # Iterate over batches of training data with a progress bar\n",
        "        inputs = inputs.to(device)  # Move the input data to the selected device\n",
        "        labels = labels.to(device)  # Move the labels to the selected device\n",
        "        optimizer.zero_grad()  # Set the gradients of model parameters to zero\n",
        "        outputs = model(inputs)  # Forward pass to get model predictions\n",
        "        loss = criterion(outputs, labels)  # Calculate the loss\n",
        "        loss.backward()  # Backpropagate the gradients\n",
        "        optimizer.step()  # Update model parameters based on gradients\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'model.pth')  # Save the trained model's state_dict to a file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE2zAqo9fe8b"
      },
      "outputs": [],
      "source": [
        "# Validate model\n",
        "model.eval()\n",
        "valid_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D76dfoumWm0W"
      },
      "source": [
        "---\n",
        "\n",
        "<center>FIN DU TP<BR>\n",
        "\n",
        "<img src=\"http://lim.univ-reunion.fr/staff/courdier/media/home_media/CC_BY_4_0.jpeg\" width=\"5%\">\n",
        "\n",
        "</center>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
