{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXl0ASQ6dFh"
      },
      "source": [
        "# **Titre du TP : Introduction à la Classification d'Images satellitaires avec les Réseaux Neuronaux Convolutifs (CNN)**\n",
        "# CORRECTION de la partie Réalisation\n",
        "\n",
        "**Description :**\n",
        "\n",
        "Dans ce TP, nous explorerons les concepts fondamentaux liés à la classification d'images en utilisant des Réseaux Neuronaux Convolutifs (CNN). Notre objectif principal sera de catégoriser des images satellites en fonction du type de terrain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4yyefqm6sxD"
      },
      "source": [
        "\n",
        "**Étapes du TP :**\n",
        "\n",
        "Dans ce TP nous allons apprendre à manipuler un réseau de neurones convolutionnel pour une tache de classification afin catégoriser des d'images satellites en fonction du type de terrain.\n",
        "\n",
        "Nous allons progresser en suivant les étapes suivantes :\n",
        "\n",
        "**Partie 1 :** Préparation de l'environnement de travail\n",
        "\n",
        "1. **Bibliothéques Deep Leraning :** Utilisation de la libraire `fastai`\n",
        "2. **Données images :** Utilisation d'images EuroSAT RGB\n",
        "\n",
        "**Partie 2 :** Appropriation\n",
        "\n",
        "1. **Utiliser un modèle Deep-Learning pré-entrainé** :  Apprendre (1) à charger un modèle de réseau de neurones existant et (2) à exécuter une tache de classification sur une images de notre choix téléchargée.\n",
        "\n",
        "2. **Manipuler un dataset** : Apprendre (1) à charger un dataset d'images dans depuis le disque pour entrainer un modèle ; (2) Savoir comment explorer les données d'un dataset.\n",
        "\n",
        "3. **Voir la structure d'un modèle** :  Apprendre (1) à créer lun modèle basé sur une architecture type (resnet50) et (2) à visualiser toutes les couches du réseau avec leurs détails.\n",
        "\n",
        "4. **Entrainer un modèle**  Apprendre (1) à réaliser une phase d'apprentissage d'un modèle ; (2) comprendre comment sauvegarder un modèle avec ces paramètres d'apprentissage ; (3) apprendre à visualiser la précision de prédiction de notre modèle et enfin (4) voir comment examiner les résultats et les erreurs du modèle.\n",
        "\n",
        "**Partie 3 :** Réalisation\n",
        "\n",
        "1. **Classifier une photo satellitaire choisie** :\n",
        "Trouver une photo satellitaire de la Réunion, telecharger là et utiliser notre modèle pour la classifier.\n",
        "\n",
        "2. **Enrichir un modèle** :\n",
        "Enrichir les classes du modèle pour prendre en considération d'autres types de terrain.\n",
        "\n",
        "3. **Bonus avec pytorch** :\n",
        "Pour les étudiants avancés, reproduire les élements de ce TP avec la bibliothèque `pytorch` qui necessite un peu plus d'entrer dans l'architecture du réseau de neurones.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1. **Classifier une image satellitaire choisie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbh_cPUSU3ha"
      },
      "source": [
        "### **Exercice 1 :** Utiliser notre modèle avec vos propres images\n",
        "\n",
        "Il vous est demandé ici :\n",
        "\n",
        "1. de trouver quelques photos satellitaires  \n",
        "sur lesquels nous pourrons appliquer notre modèle de classification.\n",
        "\n",
        "2. d'écrire le code python permettant de choisir et télécharger une image à traiter. <BR>\n",
        "Pour cela, utilisez l'utilitaire `widget` définie dans `fastai.vision.widgets` et qui permet le téléchargement d'une image avec l'instruction `widgets.FileUpload()`.\n",
        "\n",
        "3. D'appliquer notre modèle à cette image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHUUM0_xn07_"
      },
      "source": [
        "### Exercice 1. - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDtHTM1OsJ1s"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.widgets import *\n",
        "\n",
        "# Création du bouton widget pour le téléchargement\n",
        "btn_upload = widgets.FileUpload()\n",
        "btn_upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUG2XFUj4TJD"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import PILImage\n",
        "\n",
        "# l'image téléchargée est stockée dans la variable 'image'\n",
        "image = PILImage.create(btn_upload.data[-1])\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdI6TKXp4jgi"
      },
      "outputs": [],
      "source": [
        "pred, pred_idx, probs = model.predict(image)\n",
        "f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD-dYXcz5C_X"
      },
      "source": [
        "\n",
        "## 3.2. **Enrichir un modèle : Enrichir les classes du modèle pour prendre en considération d'autres types de terrain.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Exercice 2.** Enrichir ce modèle en affinant les classes à détecter **\n",
        "\n",
        "Notre modèle a été pré-entrainé pour caractériser des images dans les classes :\n",
        "- Nature\n",
        "- Humain\n",
        "\n",
        "Il vous est demandé de pouv oir maintenant considérer les classes suivantes :\n",
        "  - Forest,\n",
        "  - River,\n",
        "  - AnnualCrop,\n",
        "  - HerbaceousVegetation,\n",
        "  - Residential,\n",
        "  - PermanentCrop,\n",
        "  - Industrial,\n",
        "  - SeaLake,\n",
        "  - Pasture,\n",
        "  - Highway\n",
        "\n",
        "  Pour cela :\n",
        "  \n",
        "  **a)** Définissez une nouvelle fonction d'extraction des labels de données applicable au dataset des images EuroSAT,  \n",
        "  **b)** Testez en montrant que votre dataloader a bien étiqueté vos images de votre dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Vh92JLsCSb"
      },
      "source": [
        "### EXERCICE 2. - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import initiaux \n",
        "import fastai\n",
        "from fastai.vision.all import untar_data, Path\n",
        "from fastai.vision.all import vision_learner, resnet50, accuracy\n",
        "from fastai.vision.all import plt, L\n",
        "from fastai.vision.all import ClassificationInterpretation\n",
        "\n",
        "path = untar_data('http://madm.dfki.de/files/sentinel/EuroSAT.zip')\n",
        "Path.BASE_PATH = path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a) Nouvelle fonction d'extraction des labels de données.\n",
        "# ======================================================================\n",
        "def label_func(fname):\n",
        "    # Liste des classes appartenant à \"zones naturelles\"\n",
        "    nature_classes = ['Forest', 'River', 'AnnualCrop', 'HerbaceousVegetation', \n",
        "                      'PermanentCrop', 'SeaLake', 'Pasture', 'Residential', \n",
        "                      'Industrial', 'Highway']\n",
        "    for nc in nature_classes:\n",
        "      if (nc in fname.name):\n",
        "        return nc\n",
        "    return 'Erreur'\n",
        "\n",
        "blocks = DataBlock(\n",
        "  blocks = (ImageBlock, CategoryBlock),\n",
        "  get_items = get_image_files,\n",
        "  splitter = RandomSplitter(valid_pct=0.2, seed=0),\n",
        "  get_y = label_func,   \n",
        "  batch_tfms = aug_transforms(mult=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variante optimisée de la question a)\n",
        "# ======================================================================\n",
        "#   On utilise `RegexLabeller` pour extraire les étiquettes à partir de noms de fichiers correspondant à un motif régulier (regex).\n",
        "#   Dans notre cas, cette expression régulière capture une partie du nom du fichier qui se trouve entre le début de la chaîne\n",
        "#   et le dernier souligné avant une séquence de chiffres suivie de \".jpg\". Cette partie capturée est utilisée comme label pour les données.\n",
        "#   Par exemple, si le nom du fichier est \"nature_123.jpg\", le label extrait serait \"nature\".\n",
        "\n",
        "blocks = DataBlock(\n",
        "  blocks = (ImageBlock, CategoryBlock),\n",
        "  get_items = get_image_files,\n",
        "  splitter = RandomSplitter(valid_pct=0.2, seed=0),\n",
        "  get_y = using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),  # *** Nouvelle fonction d'extraction des labels de données\n",
        "  batch_tfms = aug_transforms(mult=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ9WM6cvp9WE"
      },
      "outputs": [],
      "source": [
        "# Création du data loader\n",
        "dataloader = blocks.dataloaders(path)\n",
        "\n",
        "# b). Test\n",
        "# ========\n",
        "dataloader.show_batch(nrows=2, ncols=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Exercice 3.** Entrainement de votre modéle, test avec vos images et analyse des résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkgVJJg4w-P-"
      },
      "source": [
        "### Exercice 3. - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2qNYSYDxBPV"
      },
      "outputs": [],
      "source": [
        "# Entrainement du modèle\n",
        "# ======================================================================\n",
        "\n",
        "# creer le modele en lui demandant une architecture de type `resnet50`\n",
        "model = vision_learner(dataloader, resnet50, metrics=accuracy)\n",
        "\n",
        "# lancer l'apprentissage 3 fois sur le dataset \n",
        "model.fine_tune(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des données suite à l'entrainement\n",
        "# ======================================================================\n",
        "\n",
        "accuracy_values = L(model.recorder.values).itemgot(2)\n",
        "plt.plot(accuracy_values);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examinun des résultats et les erreurs du modèle\n",
        "\n",
        "interpretation = ClassificationInterpretation.from_learner(model)\n",
        "interpretation.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
        "interpretation.most_confused(min_val=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4kMHbAG4uqr"
      },
      "source": [
        "# *Partie 4 du TP : Bonus*\n",
        "\n",
        "Pour les étudiants avancés, reproduire les élements de ce TP avec la bibliothèque PyTorch qui nécessite un peu plus de rentrer dans les détails de l'entrainement du réseau de neurones.\n",
        "\n",
        "Pytorch permet d'avoir plus de flexibilité dans l'entraînement, et de nombreux choix de design ne sont pas fait par défaut et nous reviennent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMO46jZjx2jc"
      },
      "source": [
        "### Exercice \"Bonus\" - CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK45ZF0VRqxM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download and unzip data\n",
        "url = 'http://madm.dfki.de/files/sentinel/EuroSAT.zip'  # Define the URL of the dataset zip file\n",
        "response = requests.get(url)  # Send an HTTP GET request to download the zip file\n",
        "zip_file = ZipFile(BytesIO(response.content))  # Create a ZipFile object from the downloaded content\n",
        "zip_file.extractall('EuroSAT')  # Extract all the contents of the zip file to the 'EuroSAT' directory\n",
        "path = '/content/EuroSAT/'  # Set the path to the extracted dataset directory\n",
        "\n",
        "# Custom Dataset Class\n",
        "class EuroSATDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir  # Set the root directory where the dataset is located\n",
        "        self.transform = transform or (lambda x: x)  # Define a data transformation (or use the identity transformation)\n",
        "        self.images = list(Path(root_dir).rglob(\"*.jpg\"))  # Create a list of image file paths in the dataset\n",
        "        self.class_to_idx = {'Human': 0, 'Nature': 1}  # Define a mapping of class names to class indices\n",
        "        self.class_to_idx_2 = {'Forest': 0, 'River': 1, 'AnnualCrop': 2, 'HerbaceousVegetation': 3, 'PermanentCrop': 4, 'SeaLake': 5, 'Pasture': 6, 'Highway': 7, 'Industrial': 8, 'Residential': 9}  # Define a more detailed class mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)  # Return the total number of images in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.images[idx])  # Get the file path of the image\n",
        "        image = self.transform(Image.open(img_name))  # Apply the transformation to the image\n",
        "        label = self.label_func(img_name)  # Get the label for the image\n",
        "        return image, label  # Return the transformed image and its label\n",
        "\n",
        "    def label_func(self, fname):\n",
        "        nature_classes = ['Forest', 'River', 'AnnualCrop', 'HerbaceousVegetation', 'PermanentCrop', 'SeaLake', 'Pasture']\n",
        "        label = 'Nature' if any(nc in fname for nc in nature_classes) else 'Human'  # Determine if the image belongs to the 'Nature' or 'Human' class based on its file path\n",
        "        return self.class_to_idx[label]  # Return the class index\n",
        "\n",
        "    def label_func_2(self, fname):\n",
        "        label = fname.split(\"/\")[-1].split(\"_\")[0]  # Extract the class label from the image file path\n",
        "        return self.class_to_idx_2[label]  # Return the class index\n",
        "\n",
        "# Transformations (basic one only)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image pixel values\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = EuroSATDataset(path, transform=transform)  # Create an instance of the EuroSATDataset with the specified transformation\n",
        "train_size = int(0.8 * len(dataset))  # Calculate the size of the training set (80% of the dataset)\n",
        "valid_size = len(dataset) - train_size  # Calculate the size of the validation set (remaining 20% of the dataset)\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])  # Split the dataset into training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Create a DataLoader for the training set\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)  # Create a DataLoader for the validation set\n",
        "\n",
        "# Model\n",
        "model = models.resnet50(pretrained=True)  # Load a pre-trained ResNet-50 model\n",
        "num_ftrs = model.fc.in_features  # Get the number of input features to the final fully connected layer\n",
        "model.fc = torch.nn.Linear(num_ftrs, 2)  # Replace the final fully connected layer with a new one for binary classification (2 classes: Nature and Human)\n",
        "\n",
        "model.requires_grad_(False)  # Set all model parameters to not require gradients\n",
        "model.fc.requires_grad_(True)  # Set the parameters of the final fully connected layer to require gradients\n",
        "\n",
        "# Training setup\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define the loss function (Cross-Entropy Loss)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Define the optimizer (Adam) and learning rate\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if a GPU is available and set the device accordingly\n",
        "\n",
        "# Training loop\n",
        "model.to(device)  # Move the model to the selected device (CPU or GPU)\n",
        "for epoch in range(3):  # Loop over a fixed number of training epochs (3 in this case)\n",
        "    model.train()  # Set the model to training mode\n",
        "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{3}', unit='batch'):  # Iterate over batches of training data with a progress bar\n",
        "        inputs = inputs.to(device)  # Move the input data to the selected device\n",
        "        labels = labels.to(device)  # Move the labels to the selected device\n",
        "        optimizer.zero_grad()  # Set the gradients of model parameters to zero\n",
        "        outputs = model(inputs)  # Forward pass to get model predictions\n",
        "        loss = criterion(outputs, labels)  # Calculate the loss\n",
        "        loss.backward()  # Backpropagate the gradients\n",
        "        optimizer.step()  # Update model parameters based on gradients\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'model.pth')  # Save the trained model's state_dict to a file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE2zAqo9fe8b"
      },
      "outputs": [],
      "source": [
        "# Validate model\n",
        "model.eval()\n",
        "valid_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D76dfoumWm0W"
      },
      "source": [
        "---\n",
        "\n",
        "<center>FIN DU TP<BR>\n",
        "\n",
        "<img src=\"http://lim.univ-reunion.fr/staff/courdier/media/home_media/CC_BY_4_0.jpeg\" width=\"5%\">\n",
        "\n",
        "</center>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
